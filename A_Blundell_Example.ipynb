{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "from MVB.nn import GaussianLinear\n",
    "from MVB.nn import GaussianConv2d\n",
    "from MVB.nn import GaussianParameter\n",
    "from MVB.nn.utils import BlundellPenalty, SetMixturePrior\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "p = 1\n",
    "\n",
    "X = torch.randn(size = (n, p))## generate X of (n=1000, p = 10) each element follows N(0,1)\n",
    "g_X = torch.exp(X) -  (torch.abs(X)) **3 \n",
    "# g_X = torch.cat((X[: , 0:1]*0,\n",
    "#                  X[: , 1:2], \n",
    "#                  X[: , 2:3]**3, \n",
    "#                  torch.exp(X[: , 3:4]),\n",
    "#                  torch.sin(2 * math.pi * X[: , 4:5]),\n",
    "#                  torch.exp(torch.abs (X[: , 5:6]))  - 2 * X[: , 5:6]) , 1)\n",
    "\n",
    "#beta = torch.empty(p, 1).uniform_(3,5)  ## generate coefficient randomly\n",
    "beta = 0.5\n",
    "error = torch.randn(size = (n, 1)) ## generate iid error from N(0,1)\n",
    "#y1 = g_X @ beta\n",
    "#y1 = X + 0.3 * torch.sin (2 * math.pi * (X + error)) + 0.4* torch.sin (4 * math.pi * (X + error))\n",
    "#y1 = (y - torch.mean(y))/ torch.std(y)\n",
    "\n",
    "y = g_X * beta + error\n",
    "\n",
    "full_dataset = torch.cat((X, y), 1)\n",
    "\n",
    "full_data_R = full_dataset.numpy()\n",
    "\n",
    "beta\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "test_dataset = torch.cat((X_test, y_test), 1)\n",
    "train_dataset = torch.cat((X_train, y_train), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv('super_X.csv')\n",
    "X_tensor = torch.tensor(X.values)\n",
    "X_tensor = X_tensor.to(torch.float32)\n",
    "p = X.shape[1]\n",
    "p\n",
    "y = pd.read_csv('super_y.csv')\n",
    "y_tensor = torch.tensor(y.values).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.1, random_state=42)\n",
    "test_dataset = torch.cat((X_test, y_test), 1)\n",
    "train_dataset = torch.cat((X_train, y_train), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 200\n",
    "\n",
    "# a neural network with 1 hidden layer that input 10 dimension x and output 1 dimension y hat with relu as activation function\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = GaussianLinear(p, h)\n",
    "        self.fc2= GaussianLinear(h,1)\n",
    "        #self.fc3= GaussianLinear(20,h)\n",
    "        #self.fc4= GaussianLinear(h,1)\n",
    "        ## eta = log sigma^2, where sigma^2 is the variance for iid noise in regression setting\n",
    "        ## to apply our framework, estimate sigma^2 is a must since we need complete likelihood information while combine\n",
    "        ## with the spike slab prior\n",
    "        self.eta = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        #out = self.relu(self.fc2(out))\n",
    "        #out = self.relu(self.fc3(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    ## customer function to compute estimated noise variance from eta\n",
    "    def getSigmaSq(self):\n",
    "        return torch.exp(self.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'example_train_data.pt')\n",
    "torch.save(test_dataset, 'example_test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code for init model\n",
    "# set prior parameters\n",
    "pi = 0.5\n",
    "tau1 = 10 ## slab sd\n",
    "tau0 = 1 ## spike sd\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "scheduler_step_size = 5\n",
    "scheduler_gamma = 0.2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNet().to(device) ## create a model object from class NeuralNet\n",
    "SetMixturePrior(model, pi, tau1, tau0) ## inject prior to this neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for init the training schema\n",
    "# set SGD parameters\n",
    "\n",
    "# set loss(model loglikelihood) type, optimizer and scheduler\n",
    "criterion = nn.MSELoss() # use MSE loss for regression problem\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                           step_size = scheduler_step_size,\n",
    "                                           gamma = scheduler_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert data into train loader and test loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 3.0909, train MSE: 1181.2370 test MSE: 1131.5659\n",
      "tensor([1.2023], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 1, Time: 6.2469, train MSE: 736.4270 test MSE: 702.4104\n",
      "tensor([1.2226], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 2, Time: 9.4073, train MSE: 731.9254 test MSE: 700.9716\n",
      "tensor([1.2442], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 3, Time: 12.5710, train MSE: 611.8353 test MSE: 589.2113\n",
      "tensor([1.2681], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 4, Time: 15.7326, train MSE: 583.5148 test MSE: 562.6883\n",
      "tensor([1.2941], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 5, Time: 18.8927, train MSE: 495.9744 test MSE: 478.4254\n",
      "tensor([1.2998], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 6, Time: 22.0578, train MSE: 481.9907 test MSE: 464.8583\n",
      "tensor([1.3064], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 7, Time: 25.2242, train MSE: 557.2577 test MSE: 541.7089\n",
      "tensor([1.3140], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 8, Time: 28.4323, train MSE: 524.2193 test MSE: 511.9573\n",
      "tensor([1.3228], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 9, Time: 31.6396, train MSE: 633.9765 test MSE: 624.2189\n",
      "tensor([1.3331], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 10, Time: 34.8482, train MSE: 487.5846 test MSE: 479.1861\n",
      "tensor([1.3353], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 11, Time: 38.0520, train MSE: 469.6586 test MSE: 461.2246\n",
      "tensor([1.3380], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 12, Time: 41.2572, train MSE: 452.0225 test MSE: 443.1920\n",
      "tensor([1.3409], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 13, Time: 44.4607, train MSE: 457.9498 test MSE: 449.2213\n",
      "tensor([1.3445], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 14, Time: 47.6695, train MSE: 454.5754 test MSE: 445.4297\n",
      "tensor([1.3485], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 15, Time: 50.8762, train MSE: 462.0839 test MSE: 453.1392\n",
      "tensor([1.3495], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 16, Time: 54.0838, train MSE: 449.8922 test MSE: 441.0511\n",
      "tensor([1.3505], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 17, Time: 57.2040, train MSE: 448.0657 test MSE: 439.2690\n",
      "tensor([1.3518], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 18, Time: 60.2802, train MSE: 461.1246 test MSE: 452.4162\n",
      "tensor([1.3533], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 19, Time: 63.3549, train MSE: 463.0697 test MSE: 454.4451\n",
      "tensor([1.3550], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 20, Time: 66.4343, train MSE: 459.0975 test MSE: 450.5134\n",
      "tensor([1.3554], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 21, Time: 69.5296, train MSE: 464.6205 test MSE: 455.9369\n",
      "tensor([1.3559], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 22, Time: 72.6130, train MSE: 457.7477 test MSE: 449.1046\n",
      "tensor([1.3564], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 23, Time: 75.7039, train MSE: 446.1023 test MSE: 437.5244\n",
      "tensor([1.3570], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 24, Time: 78.7971, train MSE: 449.6199 test MSE: 440.9673\n",
      "tensor([1.3577], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 25, Time: 81.9907, train MSE: 450.9563 test MSE: 442.3014\n",
      "tensor([1.3579], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 26, Time: 85.1996, train MSE: 450.9319 test MSE: 442.2832\n",
      "tensor([1.3581], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 27, Time: 88.4122, train MSE: 451.6201 test MSE: 442.9320\n",
      "tensor([1.3583], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 28, Time: 91.6209, train MSE: 451.4187 test MSE: 442.7201\n",
      "tensor([1.3585], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 29, Time: 94.8299, train MSE: 451.6338 test MSE: 442.9344\n",
      "tensor([1.3588], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 30, Time: 97.9237, train MSE: 451.8262 test MSE: 443.1249\n",
      "tensor([1.3589], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 31, Time: 101.0094, train MSE: 452.0431 test MSE: 443.3417\n",
      "tensor([1.3590], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 32, Time: 104.1016, train MSE: 452.2696 test MSE: 443.5681\n",
      "tensor([1.3591], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 33, Time: 107.2069, train MSE: 452.4821 test MSE: 443.7664\n",
      "tensor([1.3592], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 34, Time: 110.3590, train MSE: 452.6475 test MSE: 443.9287\n",
      "tensor([1.3593], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 35, Time: 113.5192, train MSE: 452.6498 test MSE: 443.9322\n",
      "tensor([1.3593], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 36, Time: 116.6803, train MSE: 452.5973 test MSE: 443.8817\n",
      "tensor([1.3593], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 37, Time: 120.0204, train MSE: 452.5490 test MSE: 443.8327\n",
      "tensor([1.3594], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 38, Time: 123.1097, train MSE: 452.5404 test MSE: 443.8244\n",
      "tensor([1.3594], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 39, Time: 126.1972, train MSE: 452.5399 test MSE: 443.8217\n",
      "tensor([1.3594], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 40, Time: 129.2918, train MSE: 452.5570 test MSE: 443.8387\n",
      "tensor([1.3594], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 41, Time: 132.3605, train MSE: 452.5432 test MSE: 443.8250\n",
      "tensor([1.3594], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 42, Time: 135.4281, train MSE: 452.5227 test MSE: 443.8043\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 43, Time: 138.4959, train MSE: 452.5215 test MSE: 443.8036\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 44, Time: 141.5645, train MSE: 452.5071 test MSE: 443.7889\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 45, Time: 144.6361, train MSE: 452.5070 test MSE: 443.7888\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 46, Time: 147.7040, train MSE: 452.5059 test MSE: 443.7877\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 47, Time: 150.7744, train MSE: 452.5061 test MSE: 443.7879\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 48, Time: 153.8561, train MSE: 452.5049 test MSE: 443.7867\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 49, Time: 156.9494, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 50, Time: 160.0414, train MSE: 452.5049 test MSE: 443.7867\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 51, Time: 163.1902, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 52, Time: 166.3463, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 53, Time: 169.4532, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 54, Time: 172.5462, train MSE: 452.5049 test MSE: 443.7867\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 55, Time: 175.6372, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 56, Time: 178.7277, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 57, Time: 181.8195, train MSE: 452.5049 test MSE: 443.7867\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 58, Time: 184.9786, train MSE: 452.5049 test MSE: 443.7867\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 59, Time: 188.1347, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 60, Time: 191.2953, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 61, Time: 194.4557, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 62, Time: 197.6158, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 63, Time: 200.7787, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 64, Time: 203.9430, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 65, Time: 207.1045, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 66, Time: 210.2121, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67, Time: 213.3120, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 68, Time: 216.4069, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 69, Time: 219.4789, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 70, Time: 222.5484, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 71, Time: 225.6139, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 72, Time: 228.6942, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 73, Time: 231.7638, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 74, Time: 234.8731, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 75, Time: 238.0281, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 76, Time: 241.1834, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 77, Time: 244.3395, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 78, Time: 247.4679, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 79, Time: 250.5358, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 80, Time: 253.6035, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 81, Time: 256.6721, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 82, Time: 259.8047, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 83, Time: 262.9712, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 84, Time: 266.0771, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 85, Time: 269.1683, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 86, Time: 272.2656, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 87, Time: 275.3588, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 88, Time: 278.4454, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 89, Time: 281.5112, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 90, Time: 284.5753, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 91, Time: 287.7254, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 92, Time: 290.8804, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 93, Time: 294.0564, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 94, Time: 297.1854, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 95, Time: 300.2973, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 96, Time: 303.4102, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 97, Time: 306.6153, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 98, Time: 309.7389, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 99, Time: 312.9347, train MSE: 452.5049 test MSE: 443.7868\n",
      "tensor([1.3595], device='cuda:0', grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Code for training the model\n",
    "total_step = len(train_loader)\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for observation in train_loader:\n",
    "        x = observation[:,:p].to(device)\n",
    "        y = observation[:, p].reshape((len(observation), 1)).to(device)\n",
    "        yhat = model(x)\n",
    "        \n",
    "        ## negative loglikelihood for regression + penalty\n",
    "        loss = criterion(yhat, y)/(2*model.getSigmaSq()) + torch.log(model.getSigmaSq())/2 + BlundellPenalty(model, len(train_dataset))\n",
    "        #loss = criterion(yhat, y)/(2*1) + BlundellPenalty(model, len(train_dataset))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ## report train and test mse at end of each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_MSEs = []\n",
    "        for observation in train_loader:\n",
    "            x = observation[:,:p].to(device)\n",
    "            y = observation[:, p].reshape((len(observation), 1)).to(device)\n",
    "            yhat = model(x)\n",
    "            train_MSEs.append(criterion(yhat, y).item())\n",
    "            \n",
    "        trainMSE = sum(train_MSEs)/len(train_MSEs)\n",
    "        \n",
    "        \n",
    "        test_MSEs = []\n",
    "        for observation in test_loader:\n",
    "            x = observation[:,:p].to(device)\n",
    "            y = observation[:, p].reshape((len(observation), 1)).to(device)\n",
    "            yhat = model(x)\n",
    "            test_MSEs.append(criterion(yhat, y).item())\n",
    "            \n",
    "        testMSE = sum(test_MSEs)/len(test_MSEs)\n",
    "    message = 'Epoch: {}, Time: {:.4f}, train MSE: {:.4f} test MSE: {:.4f}'.format(epoch, time.time() - start_time, trainMSE, testMSE)\n",
    "    print(message)\n",
    "    print(model.getSigmaSq())\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './results/model_bl1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_2 = torch.arange(10)\n",
    "sel_2\n",
    "train_dataset = torch.cat( (X_train[:, sel_2],y_train), 1)\n",
    "test_dataset = torch.cat( (X_test[:, sel_2],y_test), 1)\n",
    "p = train_dataset.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 3.0476, train MSE: 615.3266 test MSE: 611.4920\n",
      "tensor([4.8119], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 1, Time: 6.1030, train MSE: 588.1627 test MSE: 586.5842\n",
      "tensor([10.5126], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 2, Time: 9.1767, train MSE: 572.8918 test MSE: 575.6386\n",
      "tensor([18.0350], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 3, Time: 12.2445, train MSE: 569.8967 test MSE: 573.5735\n",
      "tensor([27.2773], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 4, Time: 15.3179, train MSE: 578.3098 test MSE: 577.9179\n",
      "tensor([38.5034], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 5, Time: 18.4389, train MSE: 563.5893 test MSE: 566.6514\n",
      "tensor([41.1316], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 6, Time: 21.5784, train MSE: 568.2980 test MSE: 573.2172\n",
      "tensor([44.2213], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 7, Time: 24.7208, train MSE: 563.2903 test MSE: 564.9672\n",
      "tensor([47.8395], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 8, Time: 27.8601, train MSE: 563.0886 test MSE: 564.8849\n",
      "tensor([52.0493], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 9, Time: 31.0007, train MSE: 561.3029 test MSE: 565.5526\n",
      "tensor([56.9529], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 10, Time: 34.1479, train MSE: 560.8655 test MSE: 564.5861\n",
      "tensor([58.0890], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 11, Time: 37.2934, train MSE: 561.7960 test MSE: 564.3142\n",
      "tensor([59.3953], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 12, Time: 40.4415, train MSE: 560.5889 test MSE: 563.5763\n",
      "tensor([60.9108], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 13, Time: 43.5796, train MSE: 560.5527 test MSE: 563.9046\n",
      "tensor([62.6563], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 14, Time: 46.7207, train MSE: 561.3230 test MSE: 565.7029\n",
      "tensor([64.6674], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 15, Time: 49.7764, train MSE: 560.0720 test MSE: 563.8394\n",
      "tensor([65.1252], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 16, Time: 52.8267, train MSE: 560.2643 test MSE: 564.2695\n",
      "tensor([65.6489], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 17, Time: 55.9621, train MSE: 560.0669 test MSE: 563.9862\n",
      "tensor([66.2443], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 18, Time: 59.1084, train MSE: 559.6129 test MSE: 563.1107\n",
      "tensor([66.9207], device='cuda:0', grad_fn=<ExpBackward>)\n",
      "Epoch: 19, Time: 62.2423, train MSE: 559.5517 test MSE: 562.7715\n",
      "tensor([67.6735], device='cuda:0', grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "h = 50\n",
    "\n",
    "# a neural network with 1 hidden layer that input 10 dimension x and output 1 dimension y hat with relu as activation function\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = GaussianLinear(p, h)\n",
    "        self.fc2= GaussianLinear(h,1)\n",
    "        #self.fc3= GaussianLinear(20,h)\n",
    "        #self.fc4= GaussianLinear(h,1)\n",
    "        ## eta = log sigma^2, where sigma^2 is the variance for iid noise in regression setting\n",
    "        ## to apply our framework, estimate sigma^2 is a must since we need complete likelihood information while combine\n",
    "        ## with the spike slab prior\n",
    "        self.eta = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        #out = self.relu(self.fc2(out))\n",
    "        #out = self.relu(self.fc3(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    ## customer function to compute estimated noise variance from eta\n",
    "    def getSigmaSq(self):\n",
    "        return torch.exp(self.eta)\n",
    "\n",
    "\n",
    "\n",
    "## code for init model\n",
    "# set prior parameters\n",
    "pi = 0.5\n",
    "tau1 = 10 ## slab sd\n",
    "tau0 = 1 ## spike sd\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "scheduler_step_size = 5\n",
    "scheduler_gamma = 0.2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNet().to(device) ## create a model object from class NeuralNet\n",
    "SetMixturePrior(model, pi, tau1, tau0) ## inject prior to this neural network\n",
    "\n",
    "## Code for init the training schema\n",
    "# set SGD parameters\n",
    "\n",
    "# set loss(model loglikelihood) type, optimizer and scheduler\n",
    "criterion = nn.MSELoss() # use MSE loss for regression problem\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                           step_size = scheduler_step_size,\n",
    "                                           gamma = scheduler_gamma)\n",
    "\n",
    "## insert data into train loader and test loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = False)\n",
    "\n",
    "## Code for training the model\n",
    "total_step = len(train_loader)\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for observation in train_loader:\n",
    "        x = observation[:,:p].to(device)\n",
    "        y = observation[:, p].reshape((len(observation), 1)).to(device)\n",
    "        yhat = model(x)\n",
    "        \n",
    "        ## negative loglikelihood for regression + penalty\n",
    "        loss = criterion(yhat, y)/(2*model.getSigmaSq()) + torch.log(model.getSigmaSq())/2 + BlundellPenalty(model, len(train_dataset))\n",
    "        #loss = criterion(yhat, y)/(2*1) + BlundellPenalty(model, len(train_dataset))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ## report train and test mse at end of each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_MSEs = []\n",
    "        for observation in train_loader:\n",
    "            x = observation[:,:p].to(device)\n",
    "            y = observation[:, p].reshape((len(observation), 1)).to(device)\n",
    "            yhat = model(x)\n",
    "            train_MSEs.append(criterion(yhat, y).item())\n",
    "            \n",
    "        trainMSE = sum(train_MSEs)/len(train_MSEs)\n",
    "        \n",
    "        \n",
    "        test_MSEs = []\n",
    "        for observation in test_loader:\n",
    "            x = observation[:,:p].to(device)\n",
    "            y = observation[:, p].reshape((len(observation), 1)).to(device)\n",
    "            yhat = model(x)\n",
    "            test_MSEs.append(criterion(yhat, y).item())\n",
    "            \n",
    "        testMSE = sum(test_MSEs)/len(test_MSEs)\n",
    "    message = 'Epoch: {}, Time: {:.4f}, train MSE: {:.4f} test MSE: {:.4f}'.format(epoch, time.time() - start_time, trainMSE, testMSE)\n",
    "    print(message)\n",
    "    print(model.getSigmaSq())\n",
    "    \n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
